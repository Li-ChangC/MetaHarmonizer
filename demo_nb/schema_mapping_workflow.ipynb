{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e13f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to the project root\n",
    "%cd /home/lcc/projects/MetaHarmonizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SchemaMapEngine class\n",
    "\n",
    "from src.Engine import SchemaMapEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the clinical data file\n",
    "\n",
    "file = \"data/schema/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89329ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SchemaMapEngine\n",
    "\n",
    "engine = SchemaMapEngine(\n",
    "    clinical_data_path=file,\n",
    "    mode=\"manual\",\n",
    "    top_k=5,\n",
    ")\n",
    "\n",
    "# Run the schema mapping\n",
    "engine.run_schema_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe08b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: evaluation\n",
    "\n",
    "# Import the schema mapping evaluation module\n",
    "from evaluation import schema_mapping_evaluation\n",
    "\n",
    "# Compute evaluation metrics\n",
    "metrics = schema_mapping_evaluation.compute_accuracy(\n",
    "    pred_file=\"data/schema_mapping_eval/test_manual.csv\",\n",
    "    truth_file=\"data/schema_mapping_eval/truth.csv\",\n",
    "    top_k=5,\n",
    "    exclude_details=(\"dict\",\"alias\"),\n",
    "    save_eval=False,\n",
    "    out_dir=\"data/schema_mapping_eval\"\n",
    ")\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
